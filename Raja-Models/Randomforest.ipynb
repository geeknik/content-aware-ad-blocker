{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score,recall_score,precision_score,classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "import json\n",
    "import os\n",
    "import slimit\n",
    "from   slimit.lexer    import Lexer\n",
    "from   slimit.parser   import Parser\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# Some of the tokens that are excluded. These are generally punctuation.\n",
    "exclude = ['LPAREN', 'RPAREN',  'PERIOD', 'SEMI', 'RBRACE', 'LBRACE', 'COMMA', 'LBRACKET', 'RBRACKET', 'COLON',  \n",
    "           'NUMBER', 'EQ', 'LT', 'NOT', 'NE', 'EQEQ', 'PLUS', 'MULT','DIV', 'VAR', 'PLUSPLUS', 'STREQ', 'STRNEQ', \n",
    "           'NULL', 'MINUSMINUS', 'PLUSEQUAL', 'MOD', 'THIS', 'MINUS', 'RSHIFT', 'BAND', 'AND', 'LSHIFT', 'BOR', 'STRING', \n",
    "           'OR',  'TRUE', 'FALSE', 'LINE_TERMINATOR', 'DIV',  'VOID', 'EXOR', 'LE', 'GT' ]test_docs  = []\n",
    "\n",
    "train_docs = []\n",
    "train_labels= []\n",
    "test_labels = []\n",
    "\n",
    "def LoadData(filename):\n",
    "\n",
    "   # Load data\n",
    "   f = open(filename, \"r\")\n",
    "\n",
    "   txt = f.read()\n",
    "\n",
    "   obj = json.loads(txt)\n",
    "\n",
    "   f.close()\n",
    "\n",
    "   return obj\n",
    "\n",
    "def Process_Files(filename, train_size, test_size):\n",
    "\n",
    "   obj = LoadData(filename)\n",
    "\n",
    "   print 'Loaded data'\n",
    "\n",
    "   analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "\n",
    "   train_count_1=0\n",
    "   train_count_0=0\n",
    "   test_count_1=0\n",
    "   test_count_0=0\n",
    "\n",
    "   count = 0\n",
    "\n",
    "   for i in itertools.count():\n",
    "     try:\n",
    "\n",
    "        x = obj[i]\n",
    "        flg = 0\n",
    "\n",
    "        if x[\"flag-any\"] == 1 :\n",
    "                       flg = 1\n",
    "\n",
    "        if flg == 1 and (train_count_1 >=  train_size/2 and test_count_1 >= test_size/2):\n",
    "           continue\n",
    "\n",
    "        if flg == 0 and (train_count_0 >=  train_size/2 and test_count_0 >= test_size/2):\n",
    "           continue\n",
    "\n",
    "\n",
    "        filename = \"../scripts/\" + x[\"sha\"] + \".js\"\n",
    "\n",
    "        filename = filename.strip('\\n')\n",
    "\n",
    "        # Open the Javascript file\n",
    "        k = open(filename, \"r+\")\n",
    "\n",
    "        # read from the file\n",
    "        strng = k.read()\n",
    "        # close the file\n",
    "        k.close()\n",
    "\n",
    "        \n",
    "        words =[]\n",
    "\n",
    "        # Tokenize  the javascript files\n",
    "        lexer = Lexer()\n",
    "        lexer.input(strng)\n",
    "\n",
    "        for token in lexer:\n",
    "           if  token.type in exclude:\n",
    "              continue\n",
    "\n",
    "           if token.type != 'ID':\n",
    "              words.append(token.type)\n",
    "           elif (token.type == 'ID' and len(token.value) > 2):\n",
    "              words.append(token.value.lower())\n",
    "\n",
    "        tags = [str(count+1)]\n",
    "\n",
    "        count += 1\n",
    "        if flg == 1:\n",
    "           if (test_count_1 < test_size/2):\n",
    "              test_labels.append(flg)\n",
    "              test_docs.append(words)\n",
    "                          test_count_1 += 1\n",
    "           else:\n",
    "              train_labels.append(flg)\n",
    "              train_docs.append(analyzedDocument(words, tags))\n",
    "              train_count_1 += 1\n",
    "        else:\n",
    "           if (test_count_0 < test_size/2):\n",
    "              test_labels.append(flg)\n",
    "              test_docs.append(words)\n",
    "              test_count_0 += 1\n",
    "           else:\n",
    "              train_labels.append(flg)\n",
    "              train_docs.append(analyzedDocument(words, tags))\n",
    "              train_count_0 += 1\n",
    "\n",
    "        if count >= train_size+test_size:\n",
    "           break\n",
    "\n",
    "     except:\n",
    "       continue\n",
    "\n",
    "   print 'Processed  %d files (train+test)' %count\n",
    "   # Train model\n",
    "   model = doc2vec.Doc2Vec(train_docs,  size=500, max_vocab_size=10000, min_count=1, window = 6, workers = 1, iter=20)\n",
    "\n",
    "   print 'Doc2Vec Trained'\n",
    "   return model\n",
    "\n",
    "def  train_classify_javascript(filename, train_size, test_size):\n",
    "  model = Process_Files(filename, train_size, test_size)\n",
    "  train_list =  []\n",
    "  test_list=[]\n",
    "  for i in range(train_size):\n",
    "     train_list.append(model.docvecs[i])\n",
    "\n",
    "  for i in range(test_size):\n",
    "     test_list.append(model.infer_vector(test_docs[i]))\n",
    "\n",
    "  train_arrays=np.array(train_list)\n",
    "  test_arrays=np.array(test_list)\n",
    "\n",
    "  # Get the vectors\n",
    "  classifier = RandomForestClassifier(n_estimators=300)\n",
    "     classifier.fit(train_arrays, train_labels)\n",
    "  print 'Random Forest Fitted'\n",
    "  predict = classifier.predict(test_arrays)\n",
    "  print classification_report(test_labels, predict)\n",
    "  print classifier.score(test_arrays, test_labels)\n",
    "\n",
    "\n",
    "train_classify_javascript('../scripts/table_flag.json', 14000, 1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[root@spark4 Models]# tail Rand.out\n",
    "Doc2Vec Trained\n",
    "Random Forest Fitted\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.78      0.87      0.82       500\n",
    "          1       0.85      0.75      0.80       500\n",
    "\n",
    "avg / total       0.81      0.81      0.81      1000\n",
    "\n",
    "0.81\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
