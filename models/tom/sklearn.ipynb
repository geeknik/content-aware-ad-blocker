{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "REPO_ROOT = \"/usr/src/app\"\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import *\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_model(model_type, train_size):\n",
    "    with open(\"%s/model-data/dataset_%s_%d.pickle\" % (REPO_ROOT, model_type, train_size), \"r\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def test_model(dataset, model_type, train_size, model, model_name):\n",
    "    model.fit(dataset[\"X_train\"], dataset[\"Y_train\"])\n",
    "    test_pred = model.predict(dataset[\"X_test\"])\n",
    "    test_y = dataset[\"Y_test\"]\n",
    "        \n",
    "    accuracy = (float(sum(test_y == test_pred))) / len(test_pred)\n",
    "    precision = (float(sum((test_y == test_pred) & (test_pred == 1)))) / float(max(1, sum(test_pred == 1)))\n",
    "    recall = (float(sum((test_y == test_pred) & (test_pred == 1)))) / float(sum(test_y == 1))\n",
    "    f1 = 2 * (precision * recall) / max(1, precision + recall)\n",
    "\n",
    "    print \"%s %s. Train set size %d. %f%% / %f%% / %f%% (%f)\" % (\n",
    "        model_type,\n",
    "        model_name,\n",
    "        train_size,\n",
    "        accuracy * 100,\n",
    "        precision * 100,\n",
    "        recall * 100,\n",
    "        f1)\n",
    "        \n",
    "    output_table.append([\n",
    "        model_type,\n",
    "        model_name,\n",
    "        train_size, \n",
    "        accuracy,\n",
    "        precision,\n",
    "        recall,\n",
    "        f1,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_mlp(input_var, input_size):\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, input_size),\n",
    "                                     input_var=input_var)\n",
    "    l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.2)\n",
    "    \n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "        l_in_drop, num_units=40,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    \n",
    "    l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.5)\n",
    "\n",
    "    l_hid2 = lasagne.layers.DenseLayer(\n",
    "        l_hid1_drop, num_units=15,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    \n",
    "    l_hid2_drop = lasagne.layers.DropoutLayer(l_hid2, p=0.5)\n",
    "    \n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "        l_hid2_drop, num_units=2,\n",
    "        nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    \n",
    "    return l_out\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert np.shape(inputs)[0] == len(targets)\n",
    "    indices = np.arange(np.shape(inputs)[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, np.shape(inputs)[0] - batchsize + 1, batchsize):\n",
    "        excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        yield inputs[excerpt].toarray(), targets[excerpt]\n",
    "    \n",
    "def test_mlp(dataset, model_type, train_size):\n",
    "    input_var = T.matrix('inputs')\n",
    "    target_var = T.lvector('targets')\n",
    "    # Create neural network model\n",
    "    network = build_mlp(input_var, np.shape(dataset[\"X_train\"])[1])\n",
    "    \n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.adam(loss, params)\n",
    "    \n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction, target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    \n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "    \n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "    \n",
    "    X_train_flat = dataset[\"X_train\"].tocsc()\n",
    "    X_test_flat = dataset[\"X_test\"].tocsc()\n",
    "\n",
    "    best_accuracy = 0\n",
    "    bad_count = 0\n",
    "    batch_size = min(200, train_size/10)\n",
    "    for epoch in xrange(999):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train_flat, dataset[\"Y_train\"], batch_size, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_test_flat, dataset[\"Y_test\"], batch_size, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "        \n",
    "        current_accuracy = val_acc / val_batches\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} took {:.3f}s - accuracy {:.2f} %\".format(\n",
    "            epoch + 1, time.time() - start_time, current_accuracy * 100))\n",
    "        \n",
    "        if current_accuracy > best_accuracy:\n",
    "            best_accuracy = current_accuracy\n",
    "            bad_count = 0\n",
    "        else:\n",
    "            bad_count += 1\n",
    "            if bad_count > 4:\n",
    "                break\n",
    "        \n",
    "    print \"%s %s. Train set size %d. %f%% / %f%% / %f%% (%f)\" % (\n",
    "            model_type,\n",
    "            \"MLP\",\n",
    "            train_size,\n",
    "            current_accuracy * 100,\n",
    "            0,\n",
    "            0,\n",
    "            0)\n",
    "    output_table.append([\n",
    "            model_type,\n",
    "            \"MLP\",\n",
    "            train_size, \n",
    "            current_accuracy,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sizes: [300, 600, 1200, 2400, 4800, 9600, 19200]\n",
      "Test size: 3588\n"
     ]
    }
   ],
   "source": [
    "with open(\"%s/model-data/metadata.pickle\" % (REPO_ROOT,), \"r\") as f:\n",
    "    size_data = pickle.load(f)\n",
    "    \n",
    "TRAIN_SIZES = size_data[\"train_sizes\"]\n",
    "TEST_SIZE = size_data[\"test_size\"]\n",
    "\n",
    "print \"Training sizes: %s\" % TRAIN_SIZES\n",
    "print \"Test size: %d\" % TEST_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegEx KNN. Train set size 300. 78.706800% / 90.297340% / 64.325530% (0.751302)\n",
      "RegEx Bernoulli. Train set size 300. 56.800446% / 53.710462% / 98.439242% (0.695002)\n",
      "RegEx SGD. Train set size 300. 80.351171% / 87.015636% / 71.348941% (0.784074)\n",
      "RegEx RandomForest. Train set size 300. 75.418060% / 72.352941% / 82.274247% (0.769953)\n",
      "RegEx LinearSVC. Train set size 300. 79.933110% / 88.193457% / 69.119287% (0.775000)\n",
      "Word2Vec KNN. Train set size 300. 72.881828% / 75.512666% / 68.829027% (0.720161)\n",
      "Word2Vec Bernoulli. Train set size 300. 69.816054% / 75.240055% / 60.307861% (0.669515)\n",
      "Word2Vec SGD. Train set size 300. 75.752508% / 72.100605% / 85.101704% (0.780635)\n",
      "Word2Vec RandomForest. Train set size 300. 77.034560% / 76.060765% / 79.824079% (0.778970)\n",
      "Word2Vec LinearSVC. Train set size 300. 71.432553% / 68.045455% / 82.297966% (0.744961)\n",
      "Doc2Vec KNN. Train set size 300. 50.641026% / 52.010050% / 34.139637% (0.355121)\n",
      "Doc2Vec Bernoulli. Train set size 300. 49.136009% / 49.870017% / 63.276526% (0.557790)\n",
      "Doc2Vec SGD. Train set size 300. 51.003344% / 50.857947% / 99.395272% (0.672869)\n",
      "Doc2Vec RandomForest. Train set size 300. 50.418060% / 50.623441% / 89.279824% (0.646111)\n",
      "Doc2Vec LinearSVC. Train set size 300. 49.804905% / 50.429799% / 58.053876% (0.539739)\n",
      "RegEx KNN. Train set size 600. 81.521739% / 91.307524% / 69.676700% (0.790389)\n",
      "RegEx Bernoulli. Train set size 600. 57.943144% / 54.399506% / 98.216276% (0.700179)\n",
      "RegEx SGD. Train set size 600. 81.744705% / 88.195842% / 73.299889% (0.800609)\n",
      "RegEx RandomForest. Train set size 600. 74.442586% / 70.366930% / 84.448161% (0.767672)\n",
      "RegEx LinearSVC. Train set size 600. 81.326644% / 89.410940% / 71.070234% (0.791925)\n",
      "Word2Vec KNN. Train set size 600. 75.557414% / 78.575739% / 71.381579% (0.748061)\n",
      "Word2Vec Bernoulli. Train set size 600. 70.680045% / 75.529101% / 62.609649% (0.684652)\n",
      "Word2Vec SGD. Train set size 600. 62.653289% / 57.918848% / 97.039474% (0.725410)\n",
      "Word2Vec RandomForest. Train set size 600. 79.375697% / 79.552890% / 79.989035% (0.797704)\n",
      "Word2Vec LinearSVC. Train set size 600. 69.175028% / 72.051597% / 64.309211% (0.679606)\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/usr/src/app/model-data/dataset_Doc2Vec_600.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a7e3b32a903c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTRAIN_SIZES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"RegEx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Word2Vec\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Doc2Vec\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#, \"AST\", \"Bigram\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         test_model(dataset, model_type, train_size,\n",
      "\u001b[0;32m<ipython-input-2-dd62dcf94be7>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_type, train_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s/model-data/dataset_%s_%d.pickle\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mREPO_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/usr/src/app/model-data/dataset_Doc2Vec_600.pickle'"
     ]
    }
   ],
   "source": [
    "output_table = []\n",
    "\n",
    "for train_size in TRAIN_SIZES:\n",
    "    for model_type in [\"RegEx\", \"AST\", \"Bigram\", \"Word2Vec\", \"Doc2Vec\"]:\n",
    "        dataset = load_model(model_type, train_size)\n",
    "\n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   KNeighborsClassifier(2), \"KNN\")\n",
    "        \n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   BernoulliNB(), \"Bernoulli\")\n",
    "\n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   linear_model.SGDClassifier(n_iter=1000, loss=\"log\"), \"SGD\")\n",
    "        \n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   RandomForestClassifier(max_depth=15, n_estimators=100, max_features=30), \"RandomForest\")\n",
    "        \n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   LinearSVC(), \"LinearSVC\")\n",
    "        \n",
    "        test_mlp(dataset, model_type, train_size)\n",
    "        \n",
    "output = (\"Model Type,Model,Training set,Accuracy,Precision,Recall,F1 score\\n\" +\n",
    "        \"\\n\".join([\",\".join([str(s) for s in row]) for row in output_table]))\n",
    "with open(\"%s/results/linear_models.csv\" % REPO_ROOT, \"w\") as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
