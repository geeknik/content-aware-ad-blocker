{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "REPO_ROOT = \"/usr/src/app\"\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import *\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_model(model_type, train_size):\n",
    "    with open(\"%s/models/data/dataset_%s_%d.pickle\" % (REPO_ROOT, model_type, train_size), \"r\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def test_model(dataset, model_type, train_size, model, model_name):\n",
    "    model.fit(dataset[\"X_train\"], dataset[\"Y_train\"])\n",
    "    test_pred = model.predict(dataset[\"X_test\"])\n",
    "    test_y = dataset[\"Y_test\"]\n",
    "        \n",
    "    accuracy = (float(sum(test_y == test_pred))) / len(test_pred)\n",
    "    precision = (float(sum((test_y == test_pred) & (test_pred == 1)))) / float(max(1, sum(test_pred == 1)))\n",
    "    recall = (float(sum((test_y == test_pred) & (test_pred == 1)))) / float(sum(test_y == 1))\n",
    "    f1 = 2 * (precision * recall) / max(1, precision + recall)\n",
    "\n",
    "    print \"%s %s. Train set size %d. %f%% / %f%% / %f%% (%f)\" % (\n",
    "        model_type,\n",
    "        model_name,\n",
    "        train_size,\n",
    "        accuracy * 100,\n",
    "        precision * 100,\n",
    "        recall * 100,\n",
    "        f1)\n",
    "        \n",
    "    output_table.append([\n",
    "        model_type,\n",
    "        model_name,\n",
    "        train_size, \n",
    "        accuracy,\n",
    "        precision,\n",
    "        recall,\n",
    "        f1,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_mlp(input_var, input_size):\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, input_size),\n",
    "                                     input_var=input_var)\n",
    "    l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.2)\n",
    "    \n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "        l_in_drop, num_units=40,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    \n",
    "    l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.5)\n",
    "\n",
    "    l_hid2 = lasagne.layers.DenseLayer(\n",
    "        l_hid1_drop, num_units=15,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    \n",
    "    l_hid2_drop = lasagne.layers.DropoutLayer(l_hid2, p=0.5)\n",
    "    \n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "        l_hid2_drop, num_units=2,\n",
    "        nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    \n",
    "    return l_out\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert np.shape(inputs)[0] == len(targets)\n",
    "    indices = np.arange(np.shape(inputs)[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, np.shape(inputs)[0] - batchsize + 1, batchsize):\n",
    "        excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        yield inputs[excerpt].toarray(), targets[excerpt]\n",
    "    \n",
    "def test_mlp(dataset, model_type, train_size):\n",
    "    input_var = T.matrix('inputs')\n",
    "    target_var = T.lvector('targets')\n",
    "    # Create neural network model\n",
    "    network = build_mlp(input_var, np.shape(dataset[\"X_train\"])[1])\n",
    "    \n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.adam(loss, params)\n",
    "    \n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction, target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    \n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "    \n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "    \n",
    "    X_train_flat = dataset[\"X_train\"].tocsc()\n",
    "    X_test_flat = dataset[\"X_test\"].tocsc()\n",
    "\n",
    "    best_accuracy = 0\n",
    "    bad_count = 0\n",
    "    batch_size = min(200, train_size/10)\n",
    "    for epoch in xrange(999):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train_flat, dataset[\"Y_train\"], batch_size, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_test_flat, dataset[\"Y_test\"], batch_size, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "        \n",
    "        current_accuracy = val_acc / val_batches\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} took {:.3f}s - accuracy {:.2f} %\".format(\n",
    "            epoch + 1, time.time() - start_time, current_accuracy * 100))\n",
    "        \n",
    "        if current_accuracy > best_accuracy:\n",
    "            best_accuracy = current_accuracy\n",
    "            bad_count = 0\n",
    "        else:\n",
    "            bad_count += 1\n",
    "            if bad_count > 4:\n",
    "                break\n",
    "        \n",
    "    print \"%s %s. Train set size %d. %f%% / %f%% / %f%% (%f)\" % (\n",
    "            model_type,\n",
    "            \"MLP\",\n",
    "            train_size,\n",
    "            current_accuracy * 100,\n",
    "            0,\n",
    "            0,\n",
    "            0)\n",
    "    output_table.append([\n",
    "            model_type,\n",
    "            \"MLP\",\n",
    "            train_size, \n",
    "            current_accuracy,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sizes: [300, 600, 1200, 2400, 4800, 9600, 19200]\n",
      "Test size: 3588\n"
     ]
    }
   ],
   "source": [
    "with open(\"%s/models/data/metadata.pickle\" % (REPO_ROOT,), \"r\") as f:\n",
    "    size_data = pickle.load(f)\n",
    "    \n",
    "TRAIN_SIZES = size_data[\"train_sizes\"]\n",
    "TEST_SIZE = size_data[\"test_size\"]\n",
    "\n",
    "print \"Training sizes: %s\" % TRAIN_SIZES\n",
    "print \"Test size: %d\" % TEST_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegEx KNN. Train set size 300. 78.177258% / 88.499619% / 64.771460% (0.747988)\n",
      "RegEx Bernoulli. Train set size 300. 56.772575% / 53.696380% / 98.383501% (0.694745)\n",
      "RegEx SGD. Train set size 300. 79.933110% / 84.645161% / 73.132664% (0.784689)\n",
      "RegEx RandomForest. Train set size 300. 75.863991% / 72.568093% / 83.166109% (0.775065)\n",
      "RegEx LinearSVC. Train set size 300. 79.960981% / 85.953177% / 71.627648% (0.781393)\n",
      "Epoch 1 took 13.546s - accuracy 76.30 %\n",
      "Epoch 2 took 13.726s - accuracy 77.90 %\n",
      "Epoch 3 took 13.879s - accuracy 78.54 %\n",
      "Epoch 4 took 13.547s - accuracy 78.46 %\n",
      "Epoch 5 took 13.719s - accuracy 79.24 %\n",
      "Epoch 6 took 13.723s - accuracy 80.67 %\n",
      "Epoch 7 took 13.508s - accuracy 81.12 %\n",
      "Epoch 8 took 13.311s - accuracy 81.34 %\n",
      "Epoch 9 took 13.351s - accuracy 81.23 %\n",
      "Epoch 10 took 13.351s - accuracy 81.23 %\n",
      "Epoch 11 took 13.530s - accuracy 81.26 %\n",
      "Epoch 12 took 13.434s - accuracy 81.62 %\n",
      "Epoch 13 took 13.921s - accuracy 81.37 %\n",
      "Epoch 14 took 13.692s - accuracy 81.32 %\n",
      "Epoch 15 took 13.670s - accuracy 81.32 %\n",
      "Epoch 16 took 13.428s - accuracy 81.37 %\n",
      "Epoch 17 took 13.511s - accuracy 81.15 %\n",
      "RegEx MLP. Train set size 300. 81.148459% / 0.000000% / 0.000000% (0.000000)\n",
      "AST KNN. Train set size 300. 76.198439% / 83.005618% / 65.886288% (0.734618)\n",
      "AST Bernoulli. Train set size 300. 56.521739% / 53.569250% / 97.881828% (0.692429)\n",
      "AST SGD. Train set size 300. 78.260870% / 80.000000% / 75.362319% (0.776119)\n",
      "AST RandomForest. Train set size 300. 74.191750% / 71.337266% / 80.880713% (0.758098)\n",
      "AST LinearSVC. Train set size 300. 78.260870% / 81.180812% / 73.578595% (0.771930)\n",
      "Epoch 1 took 15.515s - accuracy 70.42 %\n",
      "Epoch 2 took 15.317s - accuracy 71.51 %\n",
      "Epoch 3 took 15.298s - accuracy 71.93 %\n",
      "Epoch 4 took 15.291s - accuracy 76.83 %\n",
      "Epoch 5 took 15.362s - accuracy 78.74 %\n",
      "Epoch 6 took 15.288s - accuracy 79.72 %\n",
      "Epoch 7 took 15.327s - accuracy 79.97 %\n",
      "Epoch 8 took 15.290s - accuracy 80.36 %\n",
      "Epoch 9 took 15.305s - accuracy 80.20 %\n",
      "Epoch 10 took 15.283s - accuracy 80.45 %\n",
      "Epoch 11 took 15.306s - accuracy 80.42 %\n",
      "Epoch 12 took 15.753s - accuracy 80.64 %\n",
      "Epoch 13 took 15.219s - accuracy 80.64 %\n",
      "Epoch 14 took 15.632s - accuracy 80.56 %\n",
      "Epoch 15 took 15.324s - accuracy 80.50 %\n",
      "Epoch 16 took 15.686s - accuracy 80.53 %\n",
      "Epoch 17 took 16.180s - accuracy 80.53 %\n",
      "AST MLP. Train set size 300. 80.532213% / 0.000000% / 0.000000% (0.000000)\n"
     ]
    }
   ],
   "source": [
    "output_table = []\n",
    "\n",
    "for train_size in TRAIN_SIZES:\n",
    "    for model_type in [\"RegEx\", \"AST\"]:\n",
    "        dataset = load_model(model_type, train_size)\n",
    "\n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   KNeighborsClassifier(2), \"KNN\")\n",
    "        \n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   BernoulliNB(), \"Bernoulli\")\n",
    "\n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   linear_model.SGDClassifier(n_iter=1000, loss=\"log\"), \"SGD\")\n",
    "        \n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   RandomForestClassifier(max_depth=15, n_estimators=100, max_features=30), \"RandomForest\")\n",
    "        \n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   LinearSVC(), \"LinearSVC\")\n",
    "        \n",
    "        test_mlp(dataset, model_type, train_size)\n",
    "        \n",
    "output = (\"Model Type,Model,Training set,Accuracy,Precision,Recall,F1 score\\n\" +\n",
    "        \"\\n\".join([\",\".join([str(s) for s in row]) for row in output_table]))\n",
    "with open(\"%s/results/linear_models.csv\" % REPO_ROOT, \"w\") as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
